# text_evaluation.py
import os
import json
import re

from collections import OrderedDict  # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ç”¨
from functools import lru_cache
import spacy
from spacy.lang.ja import Japanese
from rapidfuzz import fuzz
import jaconv

# ã‚ãªãŸã®ç’°å¢ƒã§è‹—å­—ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ï¼ˆç›¸å¯¾ or çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«åˆã‚ã›ã¦æ›¸ãæ›ãˆã¦ãã ã•ã„ï¼‰
from ..load_surnames import load_surnames

# å½¢æ…‹ç´ è§£æžã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
nlp = spacy.load("ja_core_news_sm")  # âœ… äº‹å‰ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆ1å›žã ã‘ï¼‰
@lru_cache(maxsize=1000)
def cached_tokenize(text):
    doc = nlp(text)  # ðŸš€ ã™ã§ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® `nlp` ã‚’ä½¿ã†
    return [token.lemma_ for token in doc]

# ç°¡æ˜“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªã«ä¿å­˜ï¼‰: ãƒ†ã‚­ã‚¹ãƒˆ â†’ åˆ¤å®šçµæžœ
_eval_cache = {}

def load_offensive_dict(json_path="offensive_words.json"):
    """
    `offensive_words.json` ã‚’è¾žæ›¸ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã€‚
    {
      "categories": {
        "insults": [...],
        "defamation": [...],
        "harassment": [...],
        "threats": [...],
        ...
      },
      "names": [...]
    }
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"{json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def flatten_offensive_words(offensive_dict):
    """
    "categories" ä¸‹ã®ãƒªã‚¹ãƒˆ + "names" ãƒªã‚¹ãƒˆã‚’å…¨ã¦åˆä½“ã—ã€å˜ç´”ãª1æ¬¡å…ƒãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™ã€‚
    ä¾‹:
      {
        "categories": {
          "insults": [...],
          "defamation": [...],
          ...
        },
        "names": [...]
      }
    """
    all_words = []

    # categories ã®ã™ã¹ã¦ã®ãƒªã‚¹ãƒˆã‚’åˆä½“
    categories = offensive_dict.get("categories", {})
    for _, word_list in categories.items():
        all_words.extend(word_list)

    # "names" ã‚‚åˆä½“ã—ãŸã„å ´åˆï¼ˆã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–æ‰±ã„ã™ã‚‹ãªã‚‰ï¼‰:
    # if "names" in offensive_dict:
    #     all_words.extend(offensive_dict["names"])

    return all_words

def normalize_text(text):
    """
    å…¨è§’â†’åŠè§’ã€ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãª ã«å¤‰æ›
    """
    text = jaconv.z2h(text, kana=True, digit=True, ascii=True)
    return jaconv.kata2hira(text)

def tokenize_and_lemmatize(text):
    return cached_tokenize(text)  # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸé–¢æ•°ã‚’ç›´æŽ¥ä½¿ã†

def check_keywords_via_token(text, keywords):
    tokens = tokenize_and_lemmatize(text)
    return any(kw in tokens for kw in keywords)  # âœ… `cached_tokenize()` ã®çµæžœã‚’ä½¿ã†

@lru_cache(maxsize=1000)  # âœ… 1000ä»¶ã¾ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def check_partial_match(text, word_list, threshold=80):
    """
    æ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ã®éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯:
      - å®Œå…¨ã«å«ã¾ã‚Œã¦ã„ã‚Œã° score=100
      - fuzzy(partial_ratio) >= threshold => ãƒžãƒƒãƒã¨ã¿ãªã™
    """
    for w in word_list:
        if w in text:  # å®Œå…¨ä¸€è‡´ãªã‚‰å³ãƒžãƒƒãƒ
            return True, w, 100
        score = fuzz.ratio(w, text)  # âœ… ã‚ˆã‚Šæ­£ç¢ºãªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        if score >= threshold:
            return True, w, score
    return False, None, None

def detect_personal_accusation(text):
    pronouns_pattern = r"(ãŠå‰|ã“ã„ã¤|ã“ã®äºº|ã‚ãªãŸ|ã‚¢ãƒŠã‚¿|ã‚ã„ã¤|ã‚ã‚“ãŸ|ã‚¢ãƒ³ã‚¿|ãŠã¾ãˆ|ã‚ªãƒžã‚¨|ã‚³ã‚¤ãƒ„|ã¦ã‚ãƒ¼|ãƒ†ãƒ¡ãƒ¼|ã‚¢ã‚¤ãƒ„)"
    crime_pattern = r"(åç¤¾|æš´åŠ›å›£|è©æ¬ºå›£ä½“|è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—|çŠ¯ç½ªçµ„ç¹”|é—‡çµ„ç¹”|ãƒžãƒãƒ­ãƒ³)"
    norm = normalize_text(text)
    pattern = rf"{pronouns_pattern}.*{crime_pattern}|{crime_pattern}.*{pronouns_pattern}"
    return re.search(pattern, norm) is not None

def evaluate_text(text, offensive_dict):
    """
    (åˆ¤å®šãƒ©ãƒ™ãƒ«, è©³ç´°æ–‡å­—åˆ—) ã‚’è¿”ã™
    """
    if text in _eval_cache:
        return _eval_cache[text]

    if len(_eval_cache) > 1000:  # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        _eval_cache.popitem(last=False)  # FIFOï¼ˆå¤ã„ã‚‚ã®ã‹ã‚‰å‰Šé™¤ï¼‰

    normalized = normalize_text(text)
    all_offensive = flatten_offensive_words(offensive_dict)

    judgement = "å•é¡Œã‚ã‚Šã¾ã›ã‚“"
    detail = ""

    # (1) éƒ¨åˆ†ä¸€è‡´ã§ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–ãƒ¯ãƒ¼ãƒ‰
    found_words = []
    match, w, score = check_partial_match(normalized, all_offensive, threshold=80)
    if match:
        found_words.append((w, score))

    # (2) è‹—å­—ãƒã‚§ãƒƒã‚¯
    surnames = load_surnames()
    found_surnames = [sn for sn in surnames if sn in normalized]

    # (3) å€‹äººæ”»æ’ƒ + çŠ¯ç½ªçµ„ç¹”
    if detect_personal_accusation(text):
        judgement = "âš ï¸ å€‹äººæ”»æ’ƒ + çŠ¯ç½ªçµ„ç¹”é–¢é€£ã®è¡¨ç¾ã‚ã‚Š"
        detail = "detect_personal_accusation ãŒ True"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (4) äººåã‚ã‚Š + ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–ãƒ¯ãƒ¼ãƒ‰
    if found_words and found_surnames:
        judgement = "âš ï¸ äººå + ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–ãƒ¯ãƒ¼ãƒ‰"
        detail_list = [f"('{fw}',score={sc})" for (fw,sc) in found_words]
        detail = f"è‹—å­—={found_surnames}, words={detail_list}"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (5) ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–ã ã‘
    if found_words:
        judgement = "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã®å¯èƒ½æ€§"
        detail_list = [f"('{fw}',score={sc})" for (fw,sc) in found_words]
        detail = f"words={detail_list}"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (6) æš´åŠ›è¡¨ç¾ã®ä¾‹
    violence_keywords = ["æ®ºã™","æ­»ã­","æ®´ã‚‹","è¹´ã‚‹","åˆºã™","è½¢ã","ç„¼ã","çˆ†ç ´"]
    if check_keywords_via_token(normalized, violence_keywords):
        judgement = "âš ï¸ æš´åŠ›çš„è¡¨ç¾ã‚ã‚Š"
        detail = f"tokens in {violence_keywords}"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (7) ã„ã˜ã‚/ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆã®ä¾‹
    harassment_kws = ["ãŠå‰æ¶ˆãˆã‚","å­˜åœ¨ä¾¡å€¤ãªã„","ã„ã‚‰ãªã„äººé–“","æ­»ã‚“ã ã»ã†ãŒã„ã„","ç¤¾ä¼šã®ã‚´ãƒŸ"]
    if check_keywords_via_token(normalized, harassment_kws):
        judgement = "âš ï¸ ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆè¡¨ç¾ã‚ã‚Š"
        detail = f"tokens in {harassment_kws}"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (8) è„…è¿«ãªã©
    threat_kws = ["æ™’ã™","ç‰¹å®šã™ã‚‹","ã¶ã£å£Šã™","å¾©è®ã™ã‚‹","ç‡ƒã‚„ã™","å‘ªã†","å ±å¾©ã™ã‚‹"]
    if check_keywords_via_token(normalized, threat_kws):
        judgement = "âš ï¸ è„…è¿«è¡¨ç¾ã‚ã‚Š"
        detail = f"tokens in {threat_kws}"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # ä½•ã‚‚å½“ã¦ã¯ã¾ã‚‰ãªã‘ã‚Œã°ã€Œå•é¡Œãªã—ã€
    _eval_cache[text] = (judgement, detail)
    return judgement, detail

# ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«
if __name__ == "__main__":
    offensive_dict = load_offensive_dict("offensive_words.json")

    tests = [
        "å±±ä¸‹ã£ã¦ãƒ–ã‚¹ã ã‚ˆãª",          # äººå+ä¾®è¾± => å•é¡Œã‚ã‚Š
        "å±±ä¸‹ã¯æ”¿æ²»å®¶",                # äººåã®ã¿ => å•é¡Œãªã—
        "ãƒ–ã‚¹ã ãª",                    # äººåãªã—+ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ– => å•é¡Œã‚ã‚Š
        "æ­»ã­",                        # æš´åŠ›çš„(å½¢æ…‹ç´ ãƒã‚§ãƒƒã‚¯) => å•é¡Œã‚ã‚Š
        "ãŠå‰ã¯è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—ã¨ã¤ãªãŒã£ã¦ã‚‹",  # å€‹äººæ”»æ’ƒ+çŠ¯ç½ªçµ„ç¹” => å•é¡Œã‚ã‚Š
        "æ™®é€šã®æ–‡ç« ã§ã™"               # å•é¡Œãªã—
    ]

    for t in tests:
        res = evaluate_text(t, offensive_dict)
        print(f"å…¥åŠ›: {t} => åˆ¤å®š: {res}")
