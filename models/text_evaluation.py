# text_evaluation.py
import os
import json
import re

from collections import OrderedDict  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ç”¨
from functools import lru_cache
import spacy
from spacy.lang.ja import Japanese
from rapidfuzz import fuzz
import jaconv

# ã‚ãªãŸã®ç’°å¢ƒã§è‹—å­—ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ï¼ˆç›¸å¯¾ or çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰
from .load_surnames import load_surnames

# å½¢æ…‹ç´ è§£æžã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
nlp = spacy.load("ja_core_news_sm")  # äº‹å‰ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆ1å›žã ã‘ï¼‰

@lru_cache(maxsize=1000)
def cached_tokenize(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc]

# ç°¡æ˜“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªã«ä¿å­˜ï¼‰: ãƒ†ã‚­ã‚¹ãƒˆ â†’ åˆ¤å®šçµæžœ
_eval_cache = {}

def load_offensive_dict(json_path="offensive_words.json"):
    """
    offensive_words.json ã‚’è¾žæ›¸ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã€‚
    {
      "categories": {
        "insults": [...],
        "defamation": [...],
        "harassment": [...],
        "threats": [...],
         ...
      },
      "names": [...]
    }
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"{json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def load_whitelist(json_path="data/whitelist.json"):
    """
    whitelist.json ã‚’èª­ã¿è¾¼ã‚“ã§ set(...) ã‚’è¿”ã™ã€‚
    å½¢å¼: ["ã‚ã‚Šãˆãªã„", "èª¤æ¤œå‡ºã—ãŒã¡", ...]
    """
    if not os.path.exists(json_path):
        print(f"âš ï¸ {json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã¯ç©ºã§ã™ã€‚")
        return set()
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return set(data)
    
def flatten_offensive_words(offensive_dict):
    # offensive_dict ãŒ { "offensive": [ ... ] } ã®å ´åˆ
    if "offensive" in offensive_dict:
        return offensive_dict["offensive"]
    return []

def normalize_text(text):
    """
    å…¨è§’â†’åŠè§’ã€ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãª ã«å¤‰æ›
    """
    text = jaconv.z2h(text, kana=True, digit=True, ascii=True)
    return jaconv.kata2hira(text)

def tokenize_and_lemmatize(text):
    return cached_tokenize(text)

def check_keywords_via_token(text, keywords):
    """
    å½¢æ…‹ç´ è§£æžã—ãŸçµæžœï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ—ï¼‰ã§åˆ¤å®šã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã€‚
    text, keywords ã®ä¸¡æ–¹ã‚’ tokenize_and_lemmatize ã—ã¦ã‹ã‚‰ã€
    set(kw_tokens).issubset(set(text_tokens)) ã§åŒ…å«åˆ¤å®šã™ã‚‹ä¾‹ã€‚
    """
    text_tokens = tokenize_and_lemmatize(text)
    for kw in keywords:
        kw_tokens = tokenize_and_lemmatize(kw)
        # ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒ text_tokens ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
        if set(kw_tokens).issubset(set(text_tokens)):
            return True
    return False

def fuzzy_match_keywords(text, keywords, threshold=90):
    """
    text å†…ã«ã€keywords ã®ã„ãšã‚Œã‹ãŒéƒ¨åˆ†ä¸€è‡´ã¾ãŸã¯é¡žä¼¼åº¦ã‚¹ã‚³ã‚¢ãŒ threshold ä»¥ä¸Šã§å­˜åœ¨ã™ã‚‹ã‹åˆ¤å®š
    """
    for kw in keywords:
        score = fuzz.partial_ratio(kw, text)
        if score >= threshold:
            return True
    return False

@lru_cache(maxsize=1000)
def check_partial_match(text, word_list, threshold=80):
    """
    offensive_words.json ã«åŸºã¥ãæ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ã®éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
      - å®Œå…¨ä¸€è‡´ãªã‚‰ score=100
      - fuzz.ratio(w, text) ãŒ threshold ä»¥ä¸Šãªã‚‰ãƒžãƒƒãƒ
    """
    for w in word_list:
        if w in text:
            return True, w, 100
        score = fuzz.ratio(w, text)
        if score >= threshold:
            return True, w, score
    return False, None, None

def detect_personal_accusation(text):
    pronouns_pattern = r"(ãŠå‰|ã“ã„ã¤|ã“ã®äºº|ã‚ãªãŸ|ã‚¢ãƒŠã‚¿|ã‚ã„ã¤|ã‚ã‚“ãŸ|ã‚¢ãƒ³ã‚¿|ãŠã¾ãˆ|ã‚ªãƒžã‚¨|ã‚³ã‚¤ãƒ„|ã¦ã‚ãƒ¼|ãƒ†ãƒ¡ãƒ¼|ã‚¢ã‚¤ãƒ„)"
    crime_pattern = r"(åç¤¾|æš´åŠ›å›£|è©æ¬ºå›£ä½“|è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—|çŠ¯ç½ªçµ„ç¹”|é—‡çµ„ç¹”|ãƒžãƒãƒ­ãƒ³)"
    norm = normalize_text(text)
    pattern = rf"{pronouns_pattern}.*{crime_pattern}|{crime_pattern}.*{pronouns_pattern}"
    return re.search(pattern, norm) is not None

def evaluate_text(text, offensive_dict, whitelist=None):
    if whitelist is None:
        whitelist = set()

    if text in _eval_cache:
        return _eval_cache[text]
    if len(_eval_cache) > 1000:
        _eval_cache.popitem(last=False)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™

    # â–¼â–¼â–¼ ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› â–¼â–¼â–¼
    print(f"[DEBUG] evaluate_text called with text='{text}'")

    normalized = normalize_text(text)
    print(f"[DEBUG] normalized='{normalized}'")

    all_offensive = flatten_offensive_words(offensive_dict)
    print(f"[DEBUG] all_offensive[:34] = {all_offensive[:34]}")

    judgement = "å•é¡Œã‚ã‚Šã¾ã›ã‚“"
    detail = ""

    # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«ã‚‚ normalize_text() ã‚’é©ç”¨
    whitelist = {normalize_text(w) for w in whitelist}

    # (1) offensive_words.json ã«åŸºã¥ãéƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆ80%ä»¥ä¸Šï¼‰
    found_words = [
        word for word in all_offensive if word in normalized
    ]

    # (1.5) æ›–æ˜§æ¤œç´¢ï¼ˆ80%ä»¥ä¸Šã§ãƒžãƒƒãƒï¼‰
    fuzzy_matched_words = [
        word for word in all_offensive if fuzzy_match_keywords(normalized, [word], threshold=80)
    ]

    # (1.6) ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«ã‚ã‚‹å˜èªžã¯é™¤å¤–
    found_words = [w for w in found_words if w not in whitelist]
    fuzzy_matched_words = [w for w in fuzzy_matched_words if w not in whitelist]

    # (1.7) ã‚‚ã— offensive_words ã«å«ã¾ã‚Œã‚‹å˜èªžãŒè¦‹ã¤ã‹ã£ãŸã‚‰
    if found_words or fuzzy_matched_words:
        judgement = "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã®å¯èƒ½æ€§"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        print(f"[DEBUG] ðŸ” ãƒžãƒƒãƒã—ãŸå˜èªž: {found_words + fuzzy_matched_words}")  # ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ›
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (2) è‹—å­—ãƒã‚§ãƒƒã‚¯
    surnames = load_surnames()
    found_surnames = [sn for sn in surnames if sn in normalized]

    # (3) å€‹äººæ”»æ’ƒ + çŠ¯ç½ªçµ„ç¹”
    if detect_personal_accusation(text):
        judgement = "âš ï¸ å€‹äººæ”»æ’ƒ + çŠ¯ç½ªçµ„ç¹”é–¢é€£ã®è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (4) äººåã‚ã‚Š + offensive_words.json ç™»éŒ²ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆï¼ˆ80%ä»¥ä¸Šã§åˆ¤å®šï¼‰
    if found_words and found_surnames:
        judgement = "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (5) æš´åŠ›è¡¨ç¾ã®ä¾‹ï¼ˆç™»éŒ²å¤–ã§ã‚‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®é¡žä¼¼åº¦ãŒ80%ä»¥ä¸Šãªã‚‰æ¤œå‡ºï¼‰
    violence_keywords = ["æ®ºã™", "æ­»ã­", "æ®´ã‚‹", "è¹´ã‚‹", "åˆºã™", "è½¢ã", "ç„¼ã", "çˆ†ç ´", "æ­»ã‚“ã§ã—ã¾ãˆ"]
    if any(kw in normalized for kw in violence_keywords) or fuzzy_match_keywords(normalized, violence_keywords, threshold=80):
        judgement = "âš ï¸ æš´åŠ›çš„è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (6) ã„ã˜ã‚/ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆã®ä¾‹ï¼ˆ80%ä»¥ä¸Šã§æ¤œå‡ºï¼‰
    harassment_kws = ["ãŠå‰æ¶ˆãˆã‚", "å­˜åœ¨ä¾¡å€¤ãªã„", "ã„ã‚‰ãªã„äººé–“", "æ­»ã‚“ã ã»ã†ãŒã„ã„", "ç¤¾ä¼šã®ã‚´ãƒŸ"]
    if any(kw in normalized for kw in harassment_kws) or fuzzy_match_keywords(normalized, harassment_kws, threshold=80):
        judgement = "âš ï¸ ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆè¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (7) è„…è¿«ãªã©ï¼ˆ80%ä»¥ä¸Šã§æ¤œå‡ºï¼‰
    threat_kws = ["æ™’ã™", "ç‰¹å®šã™ã‚‹", "ã¶ã£å£Šã™", "å¾©è®ã™ã‚‹", "ç‡ƒã‚„ã™", "å‘ªã†", "å ±å¾©ã™ã‚‹"]
    if any(kw in normalized for kw in threat_kws) or fuzzy_match_keywords(normalized, threat_kws, threshold=80):
        judgement = "âš ï¸ è„…è¿«è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    _eval_cache[text] = (judgement, detail)
    return judgement, detail

    # (8) è„…è¿«ãªã©ï¼ˆ80%ä»¥ä¸Šã§æ¤œå‡ºï¼‰
    threat_kws = ["æ™’ã™", "ç‰¹å®šã™ã‚‹", "ã¶ã£å£Šã™", "å¾©è®ã™ã‚‹", "ç‡ƒã‚„ã™", "å‘ªã†", "å ±å¾©ã™ã‚‹"]
    if any(kw in normalized for kw in threat_kws) or fuzzy_match_keywords(normalized, threat_kws, threshold=80):
        judgement = "âš ï¸ è„…è¿«è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    _eval_cache[text] = (judgement, detail)
    return judgement, detail

# ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«
if __name__ == "__main__":
    offensive_dict = load_offensive_dict("offensive_words.json")
    tests = [
        "å±±ä¸‹ã£ã¦ãƒ–ã‚¹ã ã‚ˆãª",           # äººå+ä¾®è¾± => å•é¡Œã‚ã‚Š
        "å±±ä¸‹ã¯æ”¿æ²»å®¶",                 # äººåã®ã¿ => å•é¡Œãªã—
        "ãƒ–ã‚¹ã ãª",                     # äººåãªã—+ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ– => å•é¡Œã‚ã‚Š
        "æ­»ã­",                         # æš´åŠ›çš„(å½¢æ…‹ç´ ãƒã‚§ãƒƒã‚¯) => å•é¡Œã‚ã‚Š
        "ãŠå‰ã¯è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—ã¨ã¤ãªãŒã£ã¦ã‚‹",   # å€‹äººæ”»æ’ƒ+çŠ¯ç½ªçµ„ç¹” => å•é¡Œã‚ã‚Š
        "å­¦æ ¡ã«æ¥ã‚‹ãª", 
        "æ™®é€šã®æ–‡ç« ã§ã™"                # å•é¡Œãªã—
    ]
    for t in tests:
        res = evaluate_text(t, offensive_dict)
        print(f"å…¥åŠ›: {t} => åˆ¤å®š: {res}")
