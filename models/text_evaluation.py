# text_evaluation.py
import os
import json
import re

from collections import OrderedDict  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ç”¨
from functools import lru_cache
import spacy
from spacy.lang.ja import Japanese
from rapidfuzz import fuzz
import jaconv
import pykakasi

# ã‚ãªãŸã®ç’°å¢ƒã§è‹—å­—ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ï¼ˆç›¸å¯¾ or çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰
from .load_surnames import load_surnames

# å½¢æ…‹ç´ è§£æžã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
nlp = spacy.load("ja_core_news_sm")  # äº‹å‰ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆ1å›žã ã‘ï¼‰

@lru_cache(maxsize=1000)
def cached_tokenize(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc]

# ç°¡æ˜“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªã«ä¿å­˜ï¼‰: ãƒ†ã‚­ã‚¹ãƒˆ â†’ åˆ¤å®šçµæžœ
_eval_cache = {}

def load_offensive_dict(json_path="offensive_words.json"):
    """
    offensive_words.json ã‚’è¾žæ›¸ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã€‚
    {
      "categories": {
        "insults": [...],
        "defamation": [...],
        "harassment": [...],
        "threats": [...],
         ...
      },
      "names": [...]
    }
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"{json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def load_whitelist(json_path="data/whitelist.json"):
    """
    whitelist.json ã‚’èª­ã¿è¾¼ã‚“ã§ set(...) ã‚’è¿”ã™ã€‚
    å½¢å¼: ["ã‚ã‚Šãˆãªã„", "èª¤æ¤œå‡ºã—ãŒã¡", ...]
    """
    if not os.path.exists(json_path):
        print(f"âš ï¸ {json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã¯ç©ºã§ã™ã€‚")
        return set()
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return set(data)
    
def flatten_offensive_words(offensive_dict):
    """
    offensive_words.json ã® "categories" å†…ã®ãƒªã‚¹ãƒˆã‚’å…¨ã¦åˆä½“ã—ã¦ã€1æ¬¡å…ƒãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™ã€‚
    """
    all_words = []
    categories = offensive_dict.get("categories", {})
    for _, word_list in categories.items():
        all_words.extend(word_list)
    return all_words

def normalize_text(text):
    """
    - å…¨è§’â†’åŠè§’å¤‰æ›
    - ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
    - æ¼¢å­—â†’ã²ã‚‰ãŒãªå¤‰æ›
    """
    text = jaconv.z2h(text, kana=True, digit=True, ascii=True)
    text = jaconv.kata2hira(text)

    kakasi = pykakasi.kakasi()
    kakasi.setMode("J", "H")  # æ¼¢å­—ã‚’ã²ã‚‰ãŒãªã«å¤‰æ›
    kakasi.setMode("K", "H")  # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›
    kakasi.setMode("r", "Hepburn")  # ãƒ­ãƒ¼ãƒžå­—ã¯ãã®ã¾ã¾
    conv = kakasi.getConverter()
    text = conv.do(text)

    print(f"ðŸ” æ­£è¦åŒ–çµæžœ: {text}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

    return text

def tokenize_and_lemmatize(text):
    return cached_tokenize(text)

def check_keywords_via_token(text, keywords):
    tokens = tokenize_and_lemmatize(text)
    return any(kw in tokens for kw in keywords)

def fuzzy_match_keywords(text, keywords, threshold=90):
    """
    text å†…ã«ã€keywords ã®ã„ãšã‚Œã‹ãŒéƒ¨åˆ†ä¸€è‡´ã¾ãŸã¯é¡žä¼¼åº¦ã‚¹ã‚³ã‚¢ãŒ threshold ä»¥ä¸Šã§å­˜åœ¨ã™ã‚‹ã‹åˆ¤å®š
    """
    for kw in keywords:
        score = fuzz.partial_ratio(kw, text)
        if score >= threshold:
            return True
    return False

@lru_cache(maxsize=1000)
def check_partial_match(text, word_list, threshold=80):
    """
    offensive_words.json ã«åŸºã¥ãæ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ã®éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
      - å®Œå…¨ä¸€è‡´ãªã‚‰ score=100
      - fuzz.ratio(w, text) ãŒ threshold ä»¥ä¸Šãªã‚‰ãƒžãƒƒãƒ
    """
    for w in word_list:
        if w in text:
            return True, w, 100
        score = fuzz.ratio(w, text)
        if score >= threshold:
            return True, w, score
    return False, None, None

def detect_personal_accusation(text):
    pronouns_pattern = r"(ãŠå‰|ã“ã„ã¤|ã“ã®äºº|ã‚ãªãŸ|ã‚¢ãƒŠã‚¿|ã‚ã„ã¤|ã‚ã‚“ãŸ|ã‚¢ãƒ³ã‚¿|ãŠã¾ãˆ|ã‚ªãƒžã‚¨|ã‚³ã‚¤ãƒ„|ã¦ã‚ãƒ¼|ãƒ†ãƒ¡ãƒ¼|ã‚¢ã‚¤ãƒ„)"
    crime_pattern = r"(åç¤¾|æš´åŠ›å›£|è©æ¬ºå›£ä½“|è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—|çŠ¯ç½ªçµ„ç¹”|é—‡çµ„ç¹”|ãƒžãƒãƒ­ãƒ³)"
    norm = normalize_text(text)
    pattern = rf"{pronouns_pattern}.*{crime_pattern}|{crime_pattern}.*{pronouns_pattern}"
    return re.search(pattern, norm) is not None

def evaluate_text(text, offensive_dict, whitelist=None):
    if whitelist is None:
        whitelist = set()

    if text in _eval_cache:
        return _eval_cache[text]
    if len(_eval_cache) > 1000:
        _eval_cache.popitem(last=False)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™

    normalized = normalize_text(text)
    all_offensive = flatten_offensive_words(offensive_dict)

    judgement = "å•é¡Œã‚ã‚Šã¾ã›ã‚“"
    detail = ""

    # (1) offensive_words.json ã«åŸºã¥ãéƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆ70%ä»¥ä¸Šï¼‰
    found_words = []
    match, w, score = check_partial_match(normalized, tuple(all_offensive), threshold=80)
    if match:
        if w in whitelist:
            print(f"âœ… ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆå…¥ã‚Šãªã®ã§é™¤å¤–: {w}")
        else:
            found_words.append((w, score))

    # (2) è‹—å­—ãƒã‚§ãƒƒã‚¯
    surnames = load_surnames()
    found_surnames = [sn for sn in surnames if sn in normalized]

    # (3) å€‹äººæ”»æ’ƒ + çŠ¯ç½ªçµ„ç¹”
    if detect_personal_accusation(text):
        judgement = "âš ï¸ å€‹äººæ”»æ’ƒ + çŠ¯ç½ªçµ„ç¹”é–¢é€£ã®è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (4) äººåã‚ã‚Š + offensive_words.json ç™»éŒ²ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆï¼ˆ80%ä»¥ä¸Šã§åˆ¤å®šï¼‰
    if found_words and found_surnames:
        judgement = "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        detail = ("äººå + ãƒ¯ãƒ¼ãƒ‰\n"
                  "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚")
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (5) offensive_words.json ã«ç™»éŒ²ã—ã¦ã„ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆ
    if found_words:
        judgement = "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã®å¯èƒ½æ€§"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (6) æš´åŠ›è¡¨ç¾ã®ä¾‹ï¼ˆç™»éŒ²å¤–ã§ã‚‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®é¡žä¼¼åº¦ãŒ50%ä»¥ä¸Šãªã‚‰æ¤œå‡ºï¼‰
    violence_keywords = ["æ®ºã™", "æ­»ã­", "æ®´ã‚‹", "è¹´ã‚‹", "åˆºã™", "è½¢ã", "ç„¼ã", "çˆ†ç ´"]
    if any(kw in normalized for kw in violence_keywords) or fuzzy_match_keywords(normalized, violence_keywords, threshold=60):
        judgement = "âš ï¸ æš´åŠ›çš„è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (7) ã„ã˜ã‚/ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆã®ä¾‹ï¼ˆ50%ä»¥ä¸Šã§æ¤œå‡ºï¼‰
    harassment_kws = ["ãŠå‰æ¶ˆãˆã‚", "å­˜åœ¨ä¾¡å€¤ãªã„", "ã„ã‚‰ãªã„äººé–“", "æ­»ã‚“ã ã»ã†ãŒã„ã„", "ç¤¾ä¼šã®ã‚´ãƒŸ"]
    if any(kw in normalized for kw in harassment_kws) or fuzzy_match_keywords(normalized, harassment_kws, threshold=60):
        judgement = "âš ï¸ ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆè¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    # (8) è„…è¿«ãªã©ï¼ˆ50%ä»¥ä¸Šã§æ¤œå‡ºï¼‰
    threat_kws = ["æ™’ã™", "ç‰¹å®šã™ã‚‹", "ã¶ã£å£Šã™", "å¾©è®ã™ã‚‹", "ç‡ƒã‚„ã™", "å‘ªã†", "å ±å¾©ã™ã‚‹"]
    if any(kw in normalized for kw in threat_kws) or fuzzy_match_keywords(normalized, threat_kws, threshold=60):
        judgement = "âš ï¸ è„…è¿«è¡¨ç¾ã‚ã‚Š"
        detail = "â€»ã“ã®åˆ¤å®šã¯ç´„æŸã§ãã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
        _eval_cache[text] = (judgement, detail)
        return judgement, detail

    _eval_cache[text] = (judgement, detail)
    return judgement, detail

# ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«
if __name__ == "__main__":
    offensive_dict = load_offensive_dict("offensive_words.json")
    tests = [
        "å±±ä¸‹ã£ã¦ãƒ–ã‚¹ã ã‚ˆãª",           # äººå+ä¾®è¾± => å•é¡Œã‚ã‚Š
        "å±±ä¸‹ã¯æ”¿æ²»å®¶",                 # äººåã®ã¿ => å•é¡Œãªã—
        "ãƒ–ã‚¹ã ãª",                     # äººåãªã—+ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ– => å•é¡Œã‚ã‚Š
        "æ­»ã­",                         # æš´åŠ›çš„(å½¢æ…‹ç´ ãƒã‚§ãƒƒã‚¯) => å•é¡Œã‚ã‚Š
        "ãŠå‰ã¯è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—ã¨ã¤ãªãŒã£ã¦ã‚‹",   # å€‹äººæ”»æ’ƒ+çŠ¯ç½ªçµ„ç¹” => å•é¡Œã‚ã‚Š
        "æ™®é€šã®æ–‡ç« ã§ã™"                # å•é¡Œãªã—
    ]
    for t in tests:
        res = evaluate_text(t, offensive_dict)
        print(f"å…¥åŠ›: {t} => åˆ¤å®š: {res}")
