import os
import json
import re
import requests
from collections import OrderedDict
from functools import lru_cache
import spacy
from spacy.lang.ja import Japanese
from rapidfuzz import fuzz
import jaconv
import pykakasi
import io  # âœ… ãƒ¡ãƒ¢ãƒªã§ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«è¿½åŠ 

# Dropbox ã® URL
DROPBOX_URL = "https://www.dropbox.com/scl/fi/tvmzgc4vgy97nkl6v1u54/surnames.csv?rlkey=xxxxx&dl=1"

nlp = spacy.load("ja_core_news_sm")

def load_surnames():
    """âœ… Dropbox ã‹ã‚‰è‹—å­—ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã€ãƒ¡ãƒ¢ãƒªä¸Šã§å‡¦ç†"""
    print(f"ğŸ“¥ Dropbox ã‹ã‚‰ {DROPBOX_URL} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...")
    try:
        response = requests.get(DROPBOX_URL, timeout=10)
        response.raise_for_status()
        
        csv_data = response.content.decode("utf-8")  # âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã›ãšãƒ¡ãƒ¢ãƒªå‡¦ç†
        surnames = [line.strip() for line in csv_data.splitlines()]

        if not surnames:
            print("âš ï¸ è‹—å­—ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚")
        else:
            print(f"âœ… è‹—å­—ãƒªã‚¹ãƒˆã‚’ {len(surnames)} ä»¶ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼")
        
        return surnames
    except requests.RequestException as e:
        print(f"âŒ Dropbox ã‹ã‚‰ surnames.csv ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def load_whitelist(json_path="data/whitelist.json"):
    """âœ… ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(json_path):
        print(f"âš ï¸ {json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã¯ç©ºã§ã™ã€‚")
        return set()
    with open(json_path, "r", encoding="utf-8") as f:
        return set(json.load(f))

@lru_cache(maxsize=1000)
def cached_tokenize(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc]

# ãƒ†ã‚­ã‚¹ãƒˆã”ã¨ã®åˆ¤å®šçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_eval_cache = {}

def load_offensive_dict(json_path="offensive_words.json"):
    """âœ… offensive_words.json ã‚’èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"{json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return data

def flatten_offensive_words(offensive_dict):
    """âœ… offensive_words.json ã®å…¨å˜èªã‚’ãƒªã‚¹ãƒˆåŒ–"""
    all_words = []
    categories = offensive_dict.get("categories", {})
    for _, word_list in categories.items():
        all_words.extend(word_list)
    return list(set(all_words))

def normalize_text(text):
    """âœ… å…¨è§’â†’åŠè§’, ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãª, æ¼¢å­—â†’ã²ã‚‰ãŒãªå¤‰æ›"""
    kakasi = pykakasi.kakasi()
    result = kakasi.convert(text)
    return "".join([entry['hira'] for entry in result])

def fuzzy_match_keywords(text, keywords, threshold=80):
    """âœ… é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆé–¾å€¤80ï¼‰"""
    for kw in keywords:
        score = fuzz.partial_ratio(kw, text)
        if score >= threshold:
            return True
    return False

@lru_cache(maxsize=1000)
def check_partial_match(text, word_list, whitelist, threshold=80):
    """âœ… éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆè€ƒæ…®ï¼‰"""
    for w in word_list:
        if w in whitelist:
            continue  # âœ… ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«ã‚ã‚‹å ´åˆã¯ç„¡è¦–

        if w in text:
            return True, w, 100  # âœ… å®Œå…¨ä¸€è‡´ãªã‚‰å³ãƒãƒƒãƒ

        score = fuzz.partial_ratio(w, text)
        if score >= threshold:
            return True, w, score
    return False, None, None

def detect_personal_accusation(text):
    """âœ… ã€ŒãŠå‰ã€ãªã©ã®æŒ‡ç¤ºèª + ã€Œè©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—ã€ã€Œåç¤¾ã€ç­‰ãŒåŒä¸€æ–‡ã«ã‚ã‚‹ã‹ã‚’æ¤œå‡º"""
    pronouns_pattern = r"(ãŠå‰|ã“ã„ã¤|ã“ã®äºº|ã‚ãªãŸ|ã‚¢ãƒŠã‚¿|ã‚ã„ã¤|ã‚ã‚“ãŸ|ã‚¢ãƒ³ã‚¿|ãŠã¾ãˆ|ã‚ªãƒã‚¨|ã‚³ã‚¤ãƒ„|ã¦ã‚ãƒ¼|ãƒ†ãƒ¡ãƒ¼|ã‚¢ã‚¤ãƒ„)"
    crime_pattern = r"(åç¤¾|æš´åŠ›å›£|è©æ¬ºå›£ä½“|è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—|çŠ¯ç½ªçµ„ç¹”|é—‡çµ„ç¹”|ãƒãƒãƒ­ãƒ³)"

    norm = normalize_text(text)
    pattern = rf"{pronouns_pattern}.*?{crime_pattern}|{crime_pattern}.*?{pronouns_pattern}"
    
    matches = re.findall(pattern, norm)
    return bool(matches)

def evaluate_text(text, offensive_dict, whitelist=None):
    """âœ… ãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡ã—ã€å•é¡Œã®ã‚ã‚‹è¡¨ç¾ã‚’æ¤œå‡ºã™ã‚‹"""
    if whitelist is None:
        whitelist = set()

    if text in _eval_cache:
        return _eval_cache[text]

    normalized = normalize_text(text.lower())  
    all_offensive = flatten_offensive_words(offensive_dict)
    surnames = load_surnames()  # âœ… Dropbox ã‹ã‚‰å–å¾—ã™ã‚‹æ–¹å¼

    problematic = False
    detail_flags = []

    # âœ… å›ºå®šãƒªã‚¹ãƒˆã«ã‚ˆã‚‹å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    violence_keywords = {"æ®ºã™", "æ­»ã­", "æ®´ã‚‹", "è¹´ã‚‹", "çˆ†ç ´"}
    harassment_kws = {"ãŠå‰æ¶ˆãˆã‚", "å­˜åœ¨ä¾¡å€¤ãªã„", "ã„ã‚‰ãªã„äººé–“"}
    threat_kws = {"æ™’ã™", "ç‰¹å®šã™ã‚‹", "ã¶ã£å£Šã™", "å¾©è®ã™ã‚‹"}

    if normalized in violence_keywords:
        return "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", "æš´åŠ›çš„è¡¨ç¾ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰"
    if normalized in harassment_kws:
        return "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", "ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆçš„è¡¨ç¾ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰"
    if normalized in threat_kws:
        return "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", "è„…è¿«çš„è¡¨ç¾ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰"

    match, w, score = check_partial_match(normalized, tuple(all_offensive), tuple(whitelist), threshold=80)
    if match and w not in whitelist:
        problematic = True
        detail_flags.append(f"offensive_word: {w} (score={score})")

    found_surnames = [sn for sn in surnames if sn in normalized]
    if found_surnames:
        detail_flags.append(f"found_surnames: {found_surnames}")

    if fuzzy_match_keywords(normalized, violence_keywords, threshold=80):
        problematic = True
        detail_flags.append("æš´åŠ›çš„è¡¨ç¾ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")
    if fuzzy_match_keywords(normalized, harassment_kws, threshold=80):
        problematic = True
        detail_flags.append("ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆçš„è¡¨ç¾ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")
    if fuzzy_match_keywords(normalized, threat_kws, threshold=80):
        problematic = True
        detail_flags.append("è„…è¿«çš„è¡¨ç¾ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")

    if found_surnames and match:
        problematic = True
        detail_flags.append("äººå + ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–è¡¨ç¾")

    if problematic:
        judgement = "âš ï¸ ä¸€éƒ¨ã®è¡¨ç¾ãŒå•é¡Œã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        detail = " / ".join(detail_flags) + "\nâ€»ã“ã®åˆ¤å®šã¯..."
    else:
        judgement = "å•é¡Œã‚ã‚Šã¾ã›ã‚“"
        detail = ""

    _eval_cache[text] = (judgement, detail)
    return judgement, detail

if __name__ == "__main__":
    offensive_dict = load_offensive_dict("offensive_words.json")
    test_texts = [
        "å±±ä¸‹ã£ã¦ãƒ–ã‚¹ã ã‚ˆãª",
        "å±±ä¸‹ã¯æ”¿æ²»å®¶",
        "ãƒ–ã‚¹ã ãª",
        "æ­»ã­",
        "ãŠå‰ã¯è©æ¬ºã‚°ãƒ«ãƒ¼ãƒ—ã¨ã¤ãªãŒã£ã¦ã‚‹",
        "æ™®é€šã®æ–‡ç« ã§ã™",
        "å­¦æ ¡æ¥ã‚‹ãª"
    ]

    for t in test_texts:
        res = evaluate_text(t, offensive_dict)
        print(f"\nå…¥åŠ›: {t}\nåˆ¤å®š: {res}\n{'-'*40}")
